{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human detection model is on the basis of Single-Shot MultiBox Detector (SSD) [1]. To train a SSD model, the user can employ a wide range of implementations under different deep learning interfaces/libraries; however, this guide provides a brief explanation trhough a typical implementation of SSD that can be employed for human detection task. For more details, the user is refered to [2]. \n",
    "\n",
    "\n",
    "\n",
    "[1]. https://arxiv.org/abs/1512.02325\n",
    "\n",
    "[2]. https://github.com/pierluigiferrari/ssd_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this regard, at first, parameters required to create default anchor boxes and build the SSD model must be initialized. The current implementation includes creation of anchors as costum layers inside the model structure. This means that the anchors information (e.g., aspect ratios) are passed to a model builder. The model builder is a class in which the main-model and the base-model are instancitaed. The main-model is referred to the whole SSD model containing costum ancohr layers, while, the base-model has the same strcuctre of main-model without any costum layer and non-trainable layers at the top of network (i.e., reshap and softmax). It is worth to mention that the costum layers do not have any trainable parameter. After training the main-model, the weights are transfered to the base-model. The base-model is the one being transfered to the embedded device. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. General requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from models.ssd_model import build_models\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss # SSD loss function\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes # costum anchor layers\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections # decode model outputs\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder # SSD input encoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator # Data generators\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize # Augmentation chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##%% 1. Set the model configuration parameters \n",
    "img_height = 80 # Height of the input images\n",
    "img_width = 80 # Width of the input images\n",
    "img_channels = 1 # Number of color channels of the input images\n",
    "# Input normalization parameters\n",
    "intensity_mean = 0 \n",
    "intensity_range = 256\n",
    "n_classes = 1 # Number of positive classes\n",
    "scales = [0.05, 0.15, 0.3, 0.4, 0.6] # An explicit list of anchor box scaling factors. If this is passed, it will override `min_scale` and `max_scale`.\n",
    "batch_size = 8\n",
    "# The list of aspect ratios for the anchor boxes\n",
    "aspect_ratios_global = None\n",
    "aspect_ratios_per_layer = [[1./4., 1./3., 1./2., 1., 2., 3., 4.],\n",
    "                           [1./4., 1./3., 1./2., 1., 2., 3., 4.],\n",
    "                           [1./4., 1./3., 1./2., 1., 2., 3., 4.],\n",
    "                           [1./4., 1./3., 1./2., 1., 2., 3., 4.]]\n",
    "two_boxes_for_ar1 = True # Whether or not you want to generate two anchor boxes for aspect ratio 1\n",
    "steps = None # In case you'd like to set the step sizes for the anchor box grids manually; not recommended\n",
    "offsets = None # In case you'd like to set the offsets for the anchor box grids manually; not recommended\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [.1, .1, .1, .1] # The list of variances by which the encoded target coordinates are scaled\n",
    "normalize_coords = True # Whether or not the model is supposed to use coordinates relative to the image size\n",
    "n_predictor_layers = 4\n",
    "training_info_path = './main_model_training_info' # the path to export training process information\n",
    "if not os.path.isdir(training_info_path):\n",
    "    os.mkdir(training_info_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Models instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass model configuration parameters to the model builder\n",
    "\n",
    "# step 1: Clear previous models from memory.\n",
    "K.clear_session() \n",
    "image_size=(img_height, img_width, img_channels)\n",
    "model_constructor_params = {'image_size': image_size, 'n_classes': n_classes, 'mode':'training', 'training_info_path':training_info_path, \\\n",
    "                            'l2_regularization':0.0005, 'min_scale': 0.1, 'max_scale':0.9, 'scales':scales, 'n_predictor_layers':n_predictor_layers,\\\n",
    "                            'aspect_ratios_global':aspect_ratios_global,'aspect_ratios_per_layer':aspect_ratios_per_layer, 'two_boxes_for_ar1':True,\\\n",
    "                            'steps':None, 'offsets':None, 'clip_boxes':False, 'variances':variances, 'coords':'centroids',\\\n",
    "                            'normalize_coords':True, 'subtract_mean':intensity_mean,'divide_by_stddev':intensity_range,'swap_channels':False,\\\n",
    "                            'confidence_thresh':0.01,'iou_threshold':0.45, 'top_k':40, 'nms_max_output_size':400,\\\n",
    "                            'return_predictor_sizes':True,'build_base_model':True}\n",
    "\n",
    "# step 2: Pass model configuration parameters to the model builder\n",
    "constructed_models = build_models(model_constructor_params)\n",
    "main_model =constructed_models.model        # main-model being trained\n",
    "base_model = constructed_models.base_model  # base-model being transfered to the embedded device\n",
    "predictor_sizes = constructed_models.predictor_sizes # predictor sizes, required for the encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model training requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the ssd loss function \n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.)\n",
    "\n",
    "# Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile the model with ssd loss and selected optimizer\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Data generators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generators are employed to create batches of images and provide SSD required target labels, with real-time data augmentation. To put it simply, an instantiated generator needs a chain of transformation to augmnet the dataset, and a SSD-encoder to convert the labels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.1. Instantiate generators and parse the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset   = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets.\n",
    "\n",
    "# Images\n",
    "data_path   = './data'\n",
    "all_images_path = os.path.join(data_path, 'images')\n",
    "\n",
    "# Ground truth\n",
    "train_labels_filename = os.path.join(data_path, 'train_data.csv')\n",
    "val_labels_filename   = os.path.join(data_path, 'val_data.csv')\n",
    "test_labels_filename   = os.path.join(data_path, 'test_data.csv')\n",
    "\n",
    "# parce the csv files\n",
    "train_dataset.parse_csv(images_dir=all_images_path,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=all_images_path,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "\n",
    "# to evaluate the model after training\n",
    "test_dataset.parse_csv(images_dir=all_images_path,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')\n",
    "\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "test_dataset_size  = test_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n",
    "print(\"Number of images in the test dataset:\\t{:>6}\".format(test_dataset_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.2. Image processing chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is particularly important to improve detection accuracy. The chain may include both photometric and geometric transtformations. The order of transformations can be changed in the correponding class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image processing chain.\n",
    "# for train and validation datasets\n",
    "data_augmentation_chain =\\\n",
    "DataAugmentationConstantInputSize(random_brightness=(-48, 48, 0.5),\\\n",
    "                                  random_contrast=(0.5, 1.8, 0.5),\n",
    "                                  random_saturation=(0.5, 1.8, 0.5),\n",
    "                                  random_hue=(18, 0.5),\n",
    "                                  random_flip=0.5,\n",
    "                                  random_gaussian_noise = (0.5, 0., 10), # gaussine noise\n",
    "                                  random_poisson_noise  = (0.5, 20),     # poisson noise\n",
    "                                  random_salt_pepper_noise = (0.5, 0.5, 0.005), # salt&pepper or impalse noise \n",
    "                                  random_row_defect = (0.5, 1), # row defect\n",
    "                                  random_col_defect = (0.5, 1), # col defect\n",
    "                                  random_translate=((0.03,0.5), (0.03,0.5), 0.5),\n",
    "                                  random_scale=(0.5, 2.0, 0.5),\n",
    "                                  n_trials_max=3,\n",
    "                                  clip_boxes=True,\n",
    "                                  overlap_criterion='area',\n",
    "                                  bounds_box_filter=(0.3, 1.0),\n",
    "                                  bounds_validator=(0.5, 1.0),\n",
    "                                  n_boxes_min=1,\n",
    "                                  background=(127,127,127))\n",
    "\n",
    "# for test dataset\n",
    "# convert the images into the 3d arrays: (width, height, 1)\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo1Channel\n",
    "convertor = ConvertTo1Channel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.3. Instantiate SSD encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An encoder is required to encode ground truth labels into the format needed by the SSD loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_global=aspect_ratios_global,\n",
    "                                    aspect_ratios_per_layer= aspect_ratios_per_layer,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    #matching_type='bipartite',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4. Create generators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create final generators being passed to Keras fit_generator function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[data_augmentation_chain],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[data_augmentation_chain],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "test_generator = test_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convertor],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks are utilities called at certain points during model training. The user can pass a list of callbacks to get a view on internal states and statistics of the model during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model after every epoch.\n",
    "model_checkpoint = ModelCheckpoint(filepath=os.path.join(training_info_path,'main_model_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5'),\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "\n",
    "# Callback that streams epoch results to a csv file.\n",
    "csv_logger = CSVLogger(filename=os.path.join(training_info_path, 'training_log.csv'),\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "# Stop training when a monitored quantity has stopped improving.\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=41,\n",
    "                               verbose=1)\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=20,\n",
    "                                         verbose=1,\n",
    "                                         epsilon=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.000001)\n",
    "# list of callbacks \n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             early_stopping,\n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Training the main-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch numbers\n",
    "initial_epoch   = 0\n",
    "final_epoch     = 200\n",
    "\n",
    "# number of steps per epoch\n",
    "train_steps_per_epoch = int(train_dataset_size/batch_size)\n",
    "val_steps_per_epoch   = int(val_dataset_size/batch_size)\n",
    "\n",
    "\n",
    "# training\n",
    "history = main_model.fit_generator(generator=train_generator,\n",
    "                                  steps_per_epoch=train_steps_per_epoch,\n",
    "                                  epochs=final_epoch,\n",
    "                                  callbacks=callbacks,\n",
    "                                  validation_data=val_generator,\n",
    "                                  validation_steps=val_steps_per_epoch,\n",
    "                                  initial_epoch=initial_epoch)\n",
    "main_model._make_predict_function()\n",
    "\n",
    "\n",
    "#%% plot training and validation losses\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper right', prop={'size': 24});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 8. Create the base-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, a base-model is created for the embedded device. The model contains no custum anchor layers and non-trainable layers at the top of network (i.e., reshap and softmax). This enables us to create a simple tflite model which is required for the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the base model at the same path for the training info\n",
    "constructed_models.create_base_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions for some test samples, the user can employ the code below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate samples\n",
    "batch_images, batch_labels, batch_filenames = next(test_generator)\n",
    "\n",
    "# make a prediction\n",
    "y_pred = main_model.predict(batch_images)\n",
    "\n",
    "# Decode the raw prediction `y_pred`\n",
    "y_pred_decoded = decode_detections(y_pred,\n",
    "                                   confidence_thresh=0.5,\n",
    "                                   iou_threshold=0.3,\n",
    "                                   top_k=40,\n",
    "                                   normalize_coords=normalize_coords,\n",
    "                                   img_height=img_height,\n",
    "                                   img_width=img_width)\n",
    "\n",
    "# image number in the taken batch\n",
    "i = 0\n",
    "print(\"Image:\", batch_filenames[i])\n",
    "print()\n",
    "print(\"Ground truth boxes:\\n\")\n",
    "print(batch_labels[i])\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=90)\n",
    "print(\"Predicted boxes:\\n\")\n",
    "print('   class   conf xmin   ymin   xmax   ymax')\n",
    "print(y_pred_decoded[i])\n",
    "\n",
    "# Draw the predicted boxes onto the image\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(batch_images[i][:,:,0], cmap ='gray')\n",
    "\n",
    "current_axis = plt.gca()\n",
    "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist() # Set the colors for the bounding boxes\n",
    "classes = ['non-human', 'human'] # Just so we can print class names onto the image instead of IDs\n",
    "\n",
    "\n",
    "# Draw the ground truth boxes in green (omit the label for more clarity)\n",
    "for box in batch_labels[i]:\n",
    "    xmin = box[1]\n",
    "    ymin = box[2]\n",
    "    xmax = box[3]\n",
    "    ymax = box[4]\n",
    "    label = '{}'.format(classes[int(box[0])])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=5))  \n",
    "\n",
    "# Draw the predicted boxes in blue\n",
    "for box in y_pred_decoded[i]:\n",
    "    xmin = box[-4]\n",
    "    ymin = box[-3]\n",
    "    xmax = box[-2]\n",
    "    ymax = box[-1]\n",
    "    color = colors[int(box[0])]\n",
    "    label = '{}: {:.2f}'.format(classes[int(box[0])], box[1])\n",
    "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color=color, fill=False, linewidth=2))  \n",
    "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':color, 'alpha':0.5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model's performance on the test dataset, an evaluator is instantiated and employed to provide a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate a convertor, 2d images to the 3d arrays: (width, height, 1)\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo1Channel\n",
    "convertor = ConvertTo1Channel() \n",
    "\n",
    "# instantiate an evaluator using the test data set\n",
    "from eval_utils.average_precision_evaluator import Evaluator\n",
    "evaluator = Evaluator(model=model,\n",
    "                      n_classes=n_classes,\n",
    "                      data_generator=test_dataset,\n",
    "                      model_mode='training')\n",
    "\n",
    "# perform the evaluation\n",
    "results = evaluator(img_height=80,\n",
    "                    img_width=80,\n",
    "                    batch_size=32,\n",
    "                    data_generator_mode='resize',\n",
    "                    convertor = convertor,\n",
    "                    round_confidences=False,\n",
    "                    matching_iou_threshold=0.5,\n",
    "                    border_pixels='include',\n",
    "                    sorting_algorithm='quicksort',\n",
    "                    average_precision_mode='sample',\n",
    "                    num_recall_points=11,\n",
    "                    ignore_neutral_boxes=True,\n",
    "                    return_precisions=True,\n",
    "                    return_recalls=True,\n",
    "                    return_average_precisions=True,\n",
    "                    verbose=True)\n",
    "\n",
    "\n",
    "# print the evaluation results\n",
    "mean_average_precision, average_precisions, precisions, recalls = results\n",
    "classes = ['non-human', 'human']\n",
    "for i in range(1, len(average_precisions)):\n",
    "    print(\"{:<14}{:<6}{}\".format(classes[i], 'AP', round(average_precisions[i], 3)))\n",
    "    print()\n",
    "    print(\"{:<14}{:<6}{}\".format('','mAP', round(mean_average_precision, 3)))\n",
    "    \n",
    "plt.plot(recalls[n_classes], precisions[n_classes], color='blue', linewidth=1.0)\n",
    "plt.xlabel('recall', fontsize=14)\n",
    "plt.ylabel('precision', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.xticks(np.linspace(0,1,11))\n",
    "plt.yticks(np.linspace(0,1,11))\n",
    "plt.title(\"{}, AP: {:.3f}\".format(classes[n_classes], average_precisions[n_classes]), fontsize=16) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
